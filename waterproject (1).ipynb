{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Project: River Water Quality Analysis with PySpark\n",
    "\n",
    "**Objective:** This notebook demonstrates a basic Big Data analytics project using PySpark. We will load, clean, analyze, and visualize the 'river_water_resources.csv' dataset to uncover insights about water quality across different states and water body types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Install and Import Libraries\n",
    "\n",
    "First, we need to install `pyspark`, the Python library for Apache Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\rohit\\.conda\\envs\\myenv\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in c:\\users\\rohit\\.conda\\envs\\myenv\\lib\\site-packages (from pyspark) (0.10.9.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Spark Session\n",
    "\n",
    "The `SparkSession` is the entry point for any PySpark application. It allows us to create DataFrames and execute Spark commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession Created successfully\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, min, max, count, desc, when, expr\n",
    "from pyspark.sql.types import DoubleType\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WaterQualityAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession Created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the Data\n",
    "\n",
    "We will now read the `river_water_resources.csv` file into a Spark DataFrame. We'll specify `header=True` to use the first row as column names and `inferSchema=True` to automatically detect data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----+---------------+----------+---------------------+---------------------+---------------+---------------+--------+--------+----------------+----------------+---------------------+---------------------+\n",
      "|STN code| Monitoring Location|Year|Type Water Body|State Name|Temperature (C) - Min|Temperature (C) - Max|Dissolved - Min|Dissolved - Max|pH - Min|pH - Max|BOD (mg/L) - Min|BOD (mg/L) - Max|NitrateN (mg/L) - Min|NitrateN (mg/L) - Max|\n",
      "+--------+--------------------+----+---------------+----------+---------------------+---------------------+---------------+---------------+--------+--------+----------------+----------------+---------------------+---------------------+\n",
      "|    4085|RIVER JUMAR AT BI...|2022|          RIVER| JHARKHAND|                 12.0|                 29.0|            3.3|            5.2|     6.5|     6.6|               2|             2.9|                 0.37|                  1.9|\n",
      "|    2396|RIVER JUMAR AT KA...|2022|          RIVER| JHARKHAND|                 12.0|                 26.0|            5.9|            7.2|     7.5|     7.6|             1.9|             3.2|                 0.37|                  1.9|\n",
      "|    2401|RIVER AJAY AT MAS...|2022|          RIVER| JHARKHAND|                 17.0|                 36.0|            5.8|            6.6|     7.5|     7.8|             1.3|             1.6|                 0.37|                  1.9|\n",
      "|    3554|RIVER KONAR NEAR ...|2022|          RIVER| JHARKHAND|                 19.0|                 34.0|            7.4|            7.8|     7.3|     7.6|             1.8|             2.7|                 0.37|                  1.9|\n",
      "|    2390|RIVER KONAR AT TE...|2022|          RIVER| JHARKHAND|                 16.0|                 33.0|            7.4|            8.0|     7.4|     7.8|             1.3|             2.0|                 0.37|                  1.9|\n",
      "+--------+--------------------+----+---------------+----------+---------------------+---------------------+---------------+---------------+--------+--------+----------------+----------------+---------------------+---------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Define the file path\n",
    "# This assumes the file is in the same directory as the notebook.\n",
    "file_path = 'river_water_resources.csv'\n",
    "\n",
    "# Read the CSV file into a Spark DataFrame\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the first 5 rows to verify it loaded correctly\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Exploration and Cleaning\n",
    "\n",
    "Let's start by looking at the schema to understand the data types Spark has inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STN code: integer (nullable = true)\n",
      " |-- Monitoring Location: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Type Water Body: string (nullable = true)\n",
      " |-- State Name: string (nullable = true)\n",
      " |-- Temperature (C) - Min: double (nullable = true)\n",
      " |-- Temperature (C) - Max: double (nullable = true)\n",
      " |-- Dissolved - Min: string (nullable = true)\n",
      " |-- Dissolved - Max: double (nullable = true)\n",
      " |-- pH - Min: double (nullable = true)\n",
      " |-- pH - Max: double (nullable = true)\n",
      " |-- BOD (mg/L) - Min: string (nullable = true)\n",
      " |-- BOD (mg/L) - Max: double (nullable = true)\n",
      " |-- NitrateN (mg/L) - Min: string (nullable = true)\n",
      " |-- NitrateN (mg/L) - Max: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema to understand data types\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Clean Column Names\n",
    "\n",
    "The column names contain spaces, parentheses, and other special characters (e.g., `BOD (mg/L) - Max`). This makes querying difficult. We'll clean them by replacing special characters with underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Schema:\n",
      "root\n",
      " |-- STN_code: integer (nullable = true)\n",
      " |-- Monitoring_Location: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Type_Water_Body: string (nullable = true)\n",
      " |-- State_Name: string (nullable = true)\n",
      " |-- Temperature_C_Min: double (nullable = true)\n",
      " |-- Temperature_C_Max: double (nullable = true)\n",
      " |-- Dissolved_Min: string (nullable = true)\n",
      " |-- Dissolved_Max: double (nullable = true)\n",
      " |-- pH_Min: double (nullable = true)\n",
      " |-- pH_Max: double (nullable = true)\n",
      " |-- BOD_mg_L_Min: string (nullable = true)\n",
      " |-- BOD_mg_L_Max: double (nullable = true)\n",
      " |-- NitrateN_mg_L_Min: string (nullable = true)\n",
      " |-- NitrateN_mg_L_Max: double (nullable = true)\n",
      "\n",
      "DataFrame with Cleaned Column Names:\n",
      "+--------+--------------------+----+---------------+----------+-----------------+-----------------+-------------+-------------+------+------+------------+------------+-----------------+-----------------+\n",
      "|STN_code| Monitoring_Location|Year|Type_Water_Body|State_Name|Temperature_C_Min|Temperature_C_Max|Dissolved_Min|Dissolved_Max|pH_Min|pH_Max|BOD_mg_L_Min|BOD_mg_L_Max|NitrateN_mg_L_Min|NitrateN_mg_L_Max|\n",
      "+--------+--------------------+----+---------------+----------+-----------------+-----------------+-------------+-------------+------+------+------------+------------+-----------------+-----------------+\n",
      "|    4085|RIVER JUMAR AT BI...|2022|          RIVER| JHARKHAND|             12.0|             29.0|          3.3|          5.2|   6.5|   6.6|           2|         2.9|             0.37|              1.9|\n",
      "|    2396|RIVER JUMAR AT KA...|2022|          RIVER| JHARKHAND|             12.0|             26.0|          5.9|          7.2|   7.5|   7.6|         1.9|         3.2|             0.37|              1.9|\n",
      "|    2401|RIVER AJAY AT MAS...|2022|          RIVER| JHARKHAND|             17.0|             36.0|          5.8|          6.6|   7.5|   7.8|         1.3|         1.6|             0.37|              1.9|\n",
      "|    3554|RIVER KONAR NEAR ...|2022|          RIVER| JHARKHAND|             19.0|             34.0|          7.4|          7.8|   7.3|   7.6|         1.8|         2.7|             0.37|              1.9|\n",
      "|    2390|RIVER KONAR AT TE...|2022|          RIVER| JHARKHAND|             16.0|             33.0|          7.4|          8.0|   7.4|   7.8|         1.3|         2.0|             0.37|              1.9|\n",
      "+--------+--------------------+----+---------------+----------+-----------------+-----------------+-------------+-------------+------+------+------------+------------+-----------------+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Function to clean column names\n",
    "def clean_col_name(name):\n",
    "    name = re.sub(r'[^a-zA-Z0-9_]', '_', name) # Replace special chars with _\n",
    "    name = re.sub(r'__+', '_', name) # Replace multiple _ with single _\n",
    "    name = name.strip('_') # Remove leading/trailing _\n",
    "    return name\n",
    "\n",
    "# Create a new DataFrame with cleaned column names\n",
    "df_cleaned = df\n",
    "for c in df.columns:\n",
    "    df_cleaned = df_cleaned.withColumnRenamed(c, clean_col_name(c))\n",
    "\n",
    "print(\"Cleaned Schema:\")\n",
    "df_cleaned.printSchema()\n",
    "\n",
    "print(\"DataFrame with Cleaned Column Names:\")\n",
    "df_cleaned.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Handle Missing Values and Bad Data (FIXED)\n",
    "\n",
    "Real-world data is often incomplete or contains bad data (like text in a number column). \n",
    "\n",
    "1.  **Cast Numeric Columns:** We will explicitly cast our numeric columns using `try_cast`, as recommended by the error log. This will turn any malformed text (like 'BDL') into `null` without throwing an error.\n",
    "2.  **Drop Nulls:** After casting, we will drop any rows where our critical analysis columns are `null`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the count of rows before cleaning\n",
    "original_count = df_cleaned.count()\n",
    "\n",
    "# Define the columns that should be numeric\n",
    "# FIX: Corrected Temp_C_Min/Max to Temperature_C_Min/Max to match cleaned names\n",
    "numeric_cols_to_cast = [\n",
    "    \"Temperature_C_Min\", \"Temperature_C_Max\", \"Dissolved_Min\", \"Dissolved_Max\",\n",
    "    \"pH_Min\", \"pH_Max\", \"BOD_mg_L_Min\", \"BOD_mg_L_Max\",\n",
    "    \"NitrateN_mg_L_Min\", \"NitrateN_mg_L_Max\"\n",
    "]\n",
    "\n",
    "# Cast columns to Double. Malformed values like 'BDL' will become null.\n",
    "for col_name in numeric_cols_to_cast:\n",
    "    if col_name in df_cleaned.columns:\n",
    "         # Use expr(\"try_cast(... AS DOUBLE)\") as suggested by the error\n",
    "         df_cleaned = df_cleaned.withColumn(col_name, expr(f\"try_cast({col_name} AS DOUBLE)\"))\n",
    "\n",
    "# Now, define the critical columns for analysis\n",
    "# We will drop any row that has a null value in any of these columns\n",
    "critical_cols = [\n",
    "    \"State_Name\", \"Type_Water_Body\", \"BOD_mg_L_Max\", \n",
    "    \"Dissolved_Min\", \"pH_Min\", \"pH_Max\"\n",
    "]\n",
    "\n",
    "# Drop rows where any of these critical columns are null\n",
    "# (this includes original nulls AND values that failed to cast, like 'BDL')\n",
    "df_cleaned = df_cleaned.dropna(subset=critical_cols)\n",
    "\n",
    "# Get the count after dropping nulls/bad data\n",
    "cleaned_count = df_cleaned.count()\n",
    "\n",
    "print(f\"Original row count: {original_count}\")\n",
    "print(f\"Row count after dropping nulls/bad data: {cleaned_count}\")\n",
    "print(f\"Rows dropped: {original_count - cleaned_count}\")\n",
    "\n",
    "# Cache the cleaned DataFrame for faster performance on subsequent actions\n",
    "df_cleaned.cache()\n",
    "\n",
    "print(\"\\nSchema after casting and cleaning:\")\n",
    "df_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Big Data Analysis with PySpark\n",
    "\n",
    "Now we can perform distributed analysis on our cleaned data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Basic Statistics\n",
    "\n",
    "Let's get some basic summary statistics for the key water quality parameters. This should work now that all data is numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show summary statistics for important numerical columns\n",
    "# FIX: Corrected Temp_C_Max to Temperature_C_Max to match cleaned name\n",
    "df_cleaned.select(\"Temperature_C_Max\", \"Dissolved_Min\", \"pH_Max\", \"BOD_mg_L_Max\", \"NitrateN_mg_L_Max\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. State-wise Analysis\n",
    "\n",
    "Let's aggregate the data by state to see which states have the most monitoring locations and what the average water quality is like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'State_Name' and count monitoring locations\n",
    "state_counts = df_cleaned.groupBy(\"State_Name\") \\\n",
    "                         .count() \\\n",
    "                         .orderBy(desc(\"count\"))\n",
    "\n",
    "print(\"Top 10 States by Number of Monitoring Locations:\")\n",
    "state_counts.show(10)\n",
    "\n",
    "# Calculate average max BOD and min Dissolved Oxygen by state\n",
    "# High BOD (Biochemical Oxygen Demand) is generally bad.\n",
    "# Low DO (Dissolved Oxygen) is also bad.\n",
    "state_quality = df_cleaned.groupBy(\"State_Name\") \\\n",
    "                          .agg(\n",
    "                              avg(\"BOD_mg_L_Max\").alias(\"Avg_Max_BOD\"),\n",
    "                              avg(\"Dissolved_Min\").alias(\"Avg_Min_DO\"),\n",
    "                              avg(\"pH_Max\").alias(\"Avg_Max_pH\")\n",
    "                          ) \\\n",
    "                          .orderBy(desc(\"Avg_Max_BOD\"))\n",
    "\n",
    "print(\"Top 10 States by Average Max BOD (Higher is worse):\")\n",
    "state_quality.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. Water Body Type Analysis\n",
    "\n",
    "Let's analyze the data based on the type of water body (e.g., River, Drain, Canal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Type_Water_Body'\n",
    "water_body_analysis = df_cleaned.groupBy(\"Type_Water_Body\") \\\n",
    "                                .agg(\n",
    "                                    count(\"*\").alias(\"Count\"),\n",
    "                                    avg(\"pH_Max\").alias(\"Avg_Max_pH\"),\n",
    "                                    avg(\"BOD_mg_L_Max\").alias(\"Avg_Max_BOD\"),\n",
    "                                    avg(\"Dissolved_Min\").alias(\"Avg_Min_DO\")\n",
    "                                ) \\\n",
    "                                .orderBy(desc(\"Count\"))\n",
    "\n",
    "print(\"Analysis by Water Body Type:\")\n",
    "water_body_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5d. Identifying Potential Problem Areas\n",
    "\n",
    "We can use Spark's powerful filtering to quickly identify locations that fail to meet certain quality standards. \n",
    "\n",
    "Let's define a \"problem area\" as having:\n",
    "* **High BOD:** `BOD_mg_L_Max > 6.0` (indicates organic pollution)\n",
    "* **Low DO:** `Dissolved_Min < 4.0` (harmful to aquatic life)\n",
    "* **Extreme pH:** `pH_Min < 6.5` or `pH_Max > 8.5` (too acidic or alkaline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for locations with poor water quality\n",
    "problem_areas = df_cleaned.filter(\n",
    "    (col(\"BOD_mg_L_Max\") > 6.0) | \n",
    "    (col(\"Dissolved_Min\") < 4.0) | \n",
    "    (col(\"pH_Min\") < 6.5) | \n",
    "    (col(\"pH_Max\") > 8.5)\n",
    ") \\\n",
    ".select(\"Monitoring_Location\", \"State_Name\", \"Type_Water_Body\", \"BOD_mg_L_Max\", \"Dissolved_Min\", \"pH_Min\", \"pH_Max\") \\\n",
    ".orderBy(desc(\"BOD_mg_L_Max\"))\n",
    "\n",
    "print(\"Potential Problem Areas (High BOD, Low DO, or Extreme pH):\")\n",
    "problem_areas.show(20)\n",
    "\n",
    "print(f\"Total locations identified as potential problem areas: {problem_areas.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Visualization\n",
    "\n",
    "For visualization, we'll use `matplotlib`. PySpark DataFrames can be massive (billions of rows), so we can't plot them directly. \n",
    "\n",
    "The correct pattern is:\n",
    "1.  Perform heavy aggregation in Spark (as we did with `state_counts`).\n",
    "2.  The aggregated result is small (e.g., one row per state).\n",
    "3.  Convert the small Spark DataFrame to a Pandas DataFrame using `.toPandas()`.\n",
    "4.  Plot the Pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 1: Top 15 States by Monitoring Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already have `state_counts`. Let's take the top 15 states for plotting.\n",
    "state_counts_pd = state_counts.limit(15).toPandas()\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.bar(state_counts_pd['State_Name'], state_counts_pd['count'], color='skyblue')\n",
    "plt.xlabel(\"State\", fontsize=12)\n",
    "plt.ylabel(\"Number of Monitoring Locations\", fontsize=12)\n",
    "plt.title(\"Top 15 States by Monitoring Locations\", fontsize=16)\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 2: Distribution of Water Body Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have `water_body_analysis`. Let's use the 'Count' column.\n",
    "water_body_pd = water_body_analysis.select(\"Type_Water_Body\", \"Count\").toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pie(water_body_pd['Count'], labels=water_body_pd['Type_Water_Body'], autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"Distribution of Water Body Types\", fontsize=16)\n",
    "plt.axis('equal') # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 3: Top 15 States by Average Max BOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have `state_quality`. Let's plot the top 15 states with the highest Avg Max BOD.\n",
    "# We'll sort ascending for the horizontal bar chart (plots from bottom up).\n",
    "state_quality_pd = state_quality.limit(15).toPandas().sort_values(\"Avg_Max_BOD\", ascending=True)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(state_quality_pd['State_Name'], state_quality_pd['Avg_Max_BOD'], color='coral')\n",
    "plt.xlabel(\"Average Max BOD (mg/L)\", fontsize=12)\n",
    "plt.ylabel(\"State\", fontsize=12)\n",
    "plt.title(\"Top 15 States by Average Max BOD (Higher is worse)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "This project successfully demonstrated the use of PySpark for a big data analytics workflow. We loaded a CSV file, cleaned messy column names, handled missing data, performed distributed aggregations, and filtered a large dataset to find insights. Finally, we converted our aggregated Spark DataFrames to Pandas to visualize the results.\n",
    "\n",
    "This notebook can be scaled to run on a cluster with terabytes of data with no changes to the code, which is the true power of Apache Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the SparkSession to release resources\n",
    "spark.stop()\n",
    "print(\"SparkSession stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
